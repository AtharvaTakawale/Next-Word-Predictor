{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd026e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30d40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic English corpus for n-gram baseline\n",
    "# This is NOT user input and NOT transformer training data\n",
    "\n",
    "CORPUS_TEXT = \"\"\"\n",
    "Language is a system of communication used by humans to express ideas and emotions.\n",
    "Machine learning models learn patterns from data and make predictions based on probability.\n",
    "Artificial intelligence is widely used in applications such as search engines, assistants,\n",
    "and recommendation systems. Learning from examples allows models to generalize to new inputs.\n",
    "People read books, write code, and communicate using natural language every day.\n",
    "Technology continues to evolve as data and computational power increase.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef66af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = CORPUS_TEXT.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127c0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in corpus: 93\n",
      "First 20 tokens: [198, 16129, 318, 257, 1080, 286, 6946, 973, 416, 5384, 284, 4911, 4213, 290, 10825, 13, 198, 30243, 4673, 4981]\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens = tokenizer.encode(corpus_text)\n",
    "\n",
    "print(\"Total tokens in corpus:\", len(corpus_tokens))\n",
    "print(\"First 20 tokens:\", corpus_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a132f3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique bigrams: 87\n"
     ]
    }
   ],
   "source": [
    "def build_ngram(tokens, n=2):\n",
    "    return Counter(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "bigram_counts = build_ngram(corpus_tokens, n=2)\n",
    "\n",
    "print(\"Unique bigrams:\", len(bigram_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19ef3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_predict_next(\n",
    "    input_text,\n",
    "    ngram_counts,\n",
    "    n=2,\n",
    "    top_k=5\n",
    "):\n",
    "    input_tokens = tokenizer.encode(input_text.lower())\n",
    "\n",
    "    if len(input_tokens) < n - 1:\n",
    "        return []\n",
    "\n",
    "    context = tuple(input_tokens[-(n - 1):])\n",
    "\n",
    "    candidates = {\n",
    "        gram[-1]: count\n",
    "        for gram, count in ngram_counts.items()\n",
    "        if gram[:-1] == context\n",
    "    }\n",
    "\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    total = sum(candidates.values())\n",
    "\n",
    "    sorted_candidates = sorted(\n",
    "        candidates.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:top_k]\n",
    "\n",
    "    return [\n",
    "        (tokenizer.decode([token_id]).strip(), count / total)\n",
    "        for token_id, count in sorted_candidates\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c3aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: machine learning\n",
      "Predictions:\n",
      "models -> 0.5000\n",
      "from -> 0.5000\n"
     ]
    }
   ],
   "source": [
    "test_input = \"machine learning\"\n",
    "\n",
    "predictions = ngram_predict_next(\n",
    "    test_input,\n",
    "    bigram_counts,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"Input:\", test_input)\n",
    "print(\"Predictions:\")\n",
    "for word, prob in predictions:\n",
    "    print(f\"{word} -> {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a85e8dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "\n",
    "transformer_tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "transformer_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "transformer_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df18580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_predict_next(text, top_k=5):\n",
    "    input_ids = transformer_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = transformer_model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Take logits for the last token\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    probs = torch.softmax(next_token_logits, dim=0)\n",
    "\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "    return [\n",
    "        (transformer_tokenizer.decode([idx]).strip(), prob.item())\n",
    "        for idx, prob in zip(top_indices, top_probs)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27bc5e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer predictions:\n",
      "more -> 0.3203\n",
      "about -> 0.1343\n",
      "how -> 0.0869\n",
      "the -> 0.0428\n",
      "from -> 0.0375\n"
     ]
    }
   ],
   "source": [
    "test_text = \"i want to learn\"\n",
    "\n",
    "preds = transformer_predict_next(test_text)\n",
    "\n",
    "print(\"Transformer predictions:\")\n",
    "for word, prob in preds:\n",
    "    print(f\"{word} -> {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5b71cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(input_text, top_k=5):\n",
    "    return {\n",
    "        \"input\": input_text,\n",
    "        \"ngram\": ngram_predict_next(\n",
    "            input_text,\n",
    "            bigram_counts,\n",
    "            top_k=top_k\n",
    "        ),\n",
    "        \"transformer\": transformer_predict_next(\n",
    "            input_text,\n",
    "            top_k=top_k\n",
    "        )\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fde986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT: i want to learn\n",
      "\n",
      "N-GRAM:\n",
      "patterns -> 1.0000\n",
      "\n",
      "TRANSFORMER:\n",
      "more -> 0.3203\n",
      "about -> 0.1343\n",
      "how -> 0.0869\n",
      "the -> 0.0428\n",
      "from -> 0.0375\n",
      "\n",
      "INPUT: artificial intelligence is\n",
      "\n",
      "N-GRAM:\n",
      "a -> 0.5000\n",
      "widely -> 0.5000\n",
      "\n",
      "TRANSFORMER:\n",
      "a -> 0.1444\n",
      "the -> 0.0589\n",
      "not -> 0.0493\n",
      "an -> 0.0297\n",
      "one -> 0.0278\n",
      "\n",
      "INPUT: people use language to\n",
      "\n",
      "N-GRAM:\n",
      "express -> 0.2500\n",
      "general -> 0.2500\n",
      "new -> 0.2500\n",
      "evolve -> 0.2500\n",
      "\n",
      "TRANSFORMER:\n",
      "communicate -> 0.1417\n",
      "express -> 0.0639\n",
      "understand -> 0.0556\n",
      "describe -> 0.0468\n",
      "make -> 0.0309\n"
     ]
    }
   ],
   "source": [
    "sample_inputs = [\n",
    "    \"i want to learn\",\n",
    "    \"artificial intelligence is\",\n",
    "    \"people use language to\"\n",
    "]\n",
    "\n",
    "for text in sample_inputs:\n",
    "    print(\"\\nINPUT:\", text)\n",
    "\n",
    "    results = compare_models(text)\n",
    "\n",
    "    print(\"\\nN-GRAM:\")\n",
    "    for w, p in results[\"ngram\"]:\n",
    "        print(f\"{w} -> {p:.4f}\")\n",
    "\n",
    "    print(\"\\nTRANSFORMER:\")\n",
    "    for w, p in results[\"transformer\"]:\n",
    "        print(f\"{w} -> {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab2db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
